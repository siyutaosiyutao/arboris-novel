"""
Prometheusç›‘æ§æŒ‡æ ‡å®šä¹‰

ç”¨äºè¿½è¸ªå¢å¼ºæ¨¡å¼æ€§èƒ½ã€æˆæœ¬å’Œé”™è¯¯
"""
from prometheus_client import Counter, Histogram, Gauge, Summary
import time
from contextlib import contextmanager
from typing import Optional
import logging

logger = logging.getLogger(__name__)

# ==================== AIè·¯ç”±ç³»ç»ŸæŒ‡æ ‡ ====================

# AIè°ƒç”¨æ€»æ¬¡æ•°ï¼ˆæŒ‰åŠŸèƒ½ã€providerã€çŠ¶æ€åˆ†ç±»ï¼‰
ai_calls_total = Counter(
    'ai_calls_total',
    'Total AI function calls',
    ['function', 'provider', 'status']
)

# AIè°ƒç”¨è€—æ—¶åˆ†å¸ƒ
ai_duration_seconds = Histogram(
    'ai_duration_seconds',
    'AI function call duration in seconds',
    ['function', 'provider'],
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300, 600]
)

# AIè°ƒç”¨æˆæœ¬ç»Ÿè®¡
ai_cost_usd_total = Counter(
    'ai_cost_usd_total',
    'Total AI cost in USD',
    ['function', 'provider']
)

# Fallbackæ¬¡æ•°ç»Ÿè®¡
ai_fallback_total = Counter(
    'ai_fallback_total',
    'Total AI fallback occurrences',
    ['function', 'from_provider', 'to_provider']
)

# é”™è¯¯ç±»å‹ç»Ÿè®¡
ai_error_total = Counter(
    'ai_error_total',
    'Total AI errors by type',
    ['function', 'provider', 'error_type']
)

# Tokenä½¿ç”¨ç»Ÿè®¡
ai_tokens_total = Counter(
    'ai_tokens_total',
    'Total tokens used',
    ['function', 'provider', 'token_type']  # token_type: input/output
)

# å½“å‰è¿è¡Œä¸­çš„AIè°ƒç”¨
ai_calls_in_progress = Gauge(
    'ai_calls_in_progress',
    'Number of AI calls currently in progress',
    ['function']
)

# ==================== å¢å¼ºæ¨¡å¼æŒ‡æ ‡ ====================

# å¢å¼ºåˆ†ææ€»æ¬¡æ•°ï¼ˆæŒ‰çŠ¶æ€å’Œé”™è¯¯ç±»å‹åˆ†ç±»ï¼‰
enhanced_analysis_total = Counter(
    'enhanced_analysis_total',
    'Total enhanced analysis attempts',
    ['status', 'error_type', 'mode']
)

# å¢å¼ºåˆ†æè€—æ—¶åˆ†å¸ƒ
enhanced_analysis_duration = Histogram(
    'enhanced_analysis_duration_seconds',
    'Enhanced analysis duration in seconds',
    ['mode', 'feature'],
    buckets=[1, 5, 10, 30, 60, 120, 300, 600]
)

# Tokenæ¶ˆè€—ç»Ÿè®¡
token_usage_total = Counter(
    'token_usage_total',
    'Total tokens consumed',
    ['mode', 'feature', 'model']
)

# å½“å‰è¿è¡Œä¸­çš„å¢å¼ºåˆ†æä»»åŠ¡æ•°
enhanced_analysis_in_progress = Gauge(
    'enhanced_analysis_in_progress',
    'Number of enhanced analysis tasks currently running'
)

# ==================== ç« èŠ‚ç”ŸæˆæŒ‡æ ‡ ====================

# ç« èŠ‚ç”Ÿæˆæ€»æ¬¡æ•°
chapter_generation_total = Counter(
    'chapter_generation_total',
    'Total chapter generation attempts',
    ['status', 'mode']
)

# ç« èŠ‚ç”Ÿæˆè€—æ—¶
chapter_generation_duration = Histogram(
    'chapter_generation_duration_seconds',
    'Chapter generation duration in seconds',
    ['mode'],
    buckets=[10, 30, 60, 120, 300, 600, 1200]
)

# ==================== æ•°æ®è´¨é‡æŒ‡æ ‡ ====================

# è§’è‰²åŒ¹é…ç»Ÿè®¡
character_match_total = Counter(
    'character_match_total',
    'Character matching attempts',
    ['match_type', 'success']
)

# JSONè§£æå¤±è´¥ç»Ÿè®¡
json_parse_failures = Counter(
    'json_parse_failures_total',
    'JSON parsing failures',
    ['source', 'error_type']
)

# ä¸–ç•Œè§‚æ‰©å±•ç»Ÿè®¡
world_expansion_total = Counter(
    'world_expansion_total',
    'World setting expansions',
    ['category', 'confidence_level']
)

# ==================== è¾…åŠ©å‡½æ•° ====================

@contextmanager
def track_duration(metric: Histogram, **labels):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè‡ªåŠ¨è¿½è¸ªä»£ç å—æ‰§è¡Œæ—¶é—´
    
    ç”¨æ³•:
        with track_duration(enhanced_analysis_duration, mode='enhanced', feature='character_tracking'):
            # ä½ çš„ä»£ç 
            pass
    """
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        metric.labels(**labels).observe(duration)


@contextmanager
def track_in_progress(gauge: Gauge):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè¿½è¸ªå¹¶å‘ä»»åŠ¡æ•°
    
    ç”¨æ³•:
        with track_in_progress(enhanced_analysis_in_progress):
            # ä½ çš„ä»£ç 
            pass
    """
    gauge.inc()
    try:
        yield
    finally:
        gauge.dec()


def record_success(mode: str, feature: Optional[str] = None):
    """è®°å½•æˆåŠŸçš„å¢å¼ºåˆ†æ"""
    enhanced_analysis_total.labels(
        status='success',
        error_type='none',
        mode=mode
    ).inc()
    logger.info(f"âœ… {mode} æ¨¡å¼åˆ†ææˆåŠŸ: {feature or 'all'}")


def record_failure(mode: str, error_type: str, error: Exception):
    """è®°å½•å¤±è´¥çš„å¢å¼ºåˆ†æ"""
    enhanced_analysis_total.labels(
        status='failed',
        error_type=error_type,
        mode=mode
    ).inc()
    logger.error(f"âŒ {mode} æ¨¡å¼åˆ†æå¤±è´¥ ({error_type}): {error}")


def record_token_usage(mode: str, feature: str, tokens: int, model: str = 'unknown'):
    """è®°å½•Tokenæ¶ˆè€—"""
    token_usage_total.labels(
        mode=mode,
        feature=feature,
        model=model
    ).inc(tokens)
    logger.info(f"ğŸ’° Tokenæ¶ˆè€—: {mode}/{feature} = {tokens} tokens ({model})")


def record_character_match(match_type: str, success: bool):
    """
    è®°å½•è§’è‰²åŒ¹é…ç»“æœ
    
    match_type: 'exact', 'fuzzy', 'similarity', 'failed'
    """
    character_match_total.labels(
        match_type=match_type,
        success='true' if success else 'false'
    ).inc()


def record_json_parse_failure(source: str, error_type: str):
    """
    è®°å½•JSONè§£æå¤±è´¥
    
    source: 'super_analysis', 'outline_generation', etc.
    error_type: 'invalid_json', 'missing_field', 'type_error'
    """
    json_parse_failures.labels(
        source=source,
        error_type=error_type
    ).inc()
    logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {source} - {error_type}")


def record_world_expansion(category: str, confidence: float):
    """
    è®°å½•ä¸–ç•Œè§‚æ‰©å±•
    
    category: 'location', 'faction', 'item', 'rule'
    confidence: 0.0 - 1.0
    """
    if confidence >= 0.9:
        level = 'high'
    elif confidence >= 0.7:
        level = 'medium'
    else:
        level = 'low'
    
    world_expansion_total.labels(
        category=category,
        confidence_level=level
    ).inc()


# ==================== æˆæœ¬ä¼°ç®— ====================

class CostEstimator:
    """æˆæœ¬ä¼°ç®—å™¨"""
    
    # ä»·æ ¼é…ç½®ï¼ˆç¾å…ƒ/1000 tokensï¼‰
    PRICING = {
        'gpt-4': {'input': 0.03, 'output': 0.06},
        'gpt-3.5-turbo': {'input': 0.001, 'output': 0.002},
        'claude-3-sonnet': {'input': 0.003, 'output': 0.015},
    }
    
    @classmethod
    def estimate_cost(cls, model: str, input_tokens: int, output_tokens: int) -> float:
        """ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰"""
        if model not in cls.PRICING:
            logger.warning(f"æœªçŸ¥æ¨¡å‹ {model}ï¼Œä½¿ç”¨é»˜è®¤ä»·æ ¼")
            model = 'gpt-3.5-turbo'
        
        pricing = cls.PRICING[model]
        cost = (input_tokens / 1000 * pricing['input'] + 
                output_tokens / 1000 * pricing['output'])
        return round(cost, 6)
    
    @classmethod
    def estimate_chapter_cost(cls, mode: str, model: str = 'gpt-3.5-turbo') -> dict:
        """ä¼°ç®—å•ç« æˆæœ¬"""
        if mode == 'basic':
            # åŸºç¡€æ¨¡å¼ï¼š2.5æ¬¡è°ƒç”¨
            # å¤§çº²0.5æ¬¡(1000 input, 500 output) + å†…å®¹2æ¬¡(2000 input, 3000 output)
            input_tokens = 1000 * 0.5 + 2000 * 2
            output_tokens = 500 * 0.5 + 3000 * 2
        else:  # enhanced
            # å¢å¼ºæ¨¡å¼ï¼š3.6æ¬¡è°ƒç”¨
            # åŸºç¡€2.5æ¬¡ + è¶…çº§åˆ†æ1.1æ¬¡(5000 input, 2000 output)
            input_tokens = 1000 * 0.5 + 2000 * 2 + 5000 * 1.1
            output_tokens = 500 * 0.5 + 3000 * 2 + 2000 * 1.1
        
        cost = cls.estimate_cost(model, int(input_tokens), int(output_tokens))
        
        return {
            'mode': mode,
            'model': model,
            'input_tokens': int(input_tokens),
            'output_tokens': int(output_tokens),
            'total_tokens': int(input_tokens + output_tokens),
            'cost_usd': cost,
            'cost_cny': round(cost * 7.2, 4)  # å‡è®¾æ±‡ç‡7.2
        }

